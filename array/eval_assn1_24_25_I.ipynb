{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LciKvMEzH9I-"
      },
      "outputs": [],
      "source": [
        "import sqlite3 as sql\n",
        "from contextlib import closing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time as tm\n",
        "import pickle\n",
        "import os\n",
        "import csv\n",
        "import importlib\n",
        "import execute  # Import module without specifying the function\n",
        "importlib.reload(execute)  # Force reload\n",
        "import index  # Import module without specifying the function\n",
        "importlib.reload(index) # Force reload\n",
        "from execute import my_execute\n",
        "from index import my_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGdRLObRKAlM"
      },
      "source": [
        "sqlite3 does not support contextual closing natively (yet). Using a super-elegant workaround proposed by erlendaasland\\\n",
        "https://discuss.python.org/t/implicitly-close-sqlite3-connections-with-context-managers/33320/3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "POoHIPqtIbVz"
      },
      "outputs": [],
      "source": [
        "def safe_tran( db_name, query ):\n",
        "  with closing( sql.connect( db_name ) ) as conn:\n",
        "    cur = conn.execute( query )\n",
        "    cols = [ col[0] for col in cur.description ]\n",
        "    df = pd.DataFrame.from_records( cur, columns = cols )\n",
        "    return df\n",
        "\n",
        "\n",
        "db_name = \"public.db\"\n",
        "get_gold_results = lambda query: safe_tran( db_name, query )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EY9Q-50DKTdE"
      },
      "outputs": [],
      "source": [
        "def make_sqlite_query( clause ):\n",
        "  query = \"SELECT id FROM tbl WHERE \"\n",
        "  query += \" AND \".join( [ ' '.join( pred ) for pred in clause ] )\n",
        "  return query\n",
        "\n",
        "def eval_results( clause, disk, idx_stat ):\n",
        "  # Execute the query on an actual DB\n",
        "  df_gold = get_gold_results( make_sqlite_query( clause ) )\n",
        "\n",
        "  # Execute the query using the index and time it\n",
        "  tic = tm.perf_counter()\n",
        "  diskloc_list = my_execute( clause, idx_stat )\n",
        "  toc = tm.perf_counter()\n",
        "  t_idx = toc - tic\n",
        "\n",
        "  # Do sanity checks on the returned locations -- dont want any buffer overflow attacks :)\n",
        "  diskloc_list = np.minimum( np.maximum( diskloc_list, 0 ), len( disk ) - 1 )\n",
        "\n",
        "  # Find the seek and read time requried to retrieve records from the virtual disk\n",
        "  diffs = diskloc_list[ 1: ] - diskloc_list[ :-1 ]\n",
        "  # Take care of cases where we need to loop back to reach a record\n",
        "  diffs[ diffs <= 0 ] += len( disk )\n",
        "  t_seek = diffs.sum()\n",
        "  t_read = len( diskloc_list )\n",
        "  # Sanity check\n",
        "  assert( t_seek >= t_read - 1 )\n",
        "  t_seek -= t_read - 1\n",
        "  # Take care of pesky edge cases\n",
        "  if t_read == 0:\n",
        "    t_seek = 0\n",
        "\n",
        "  # Get hold of the tuples chosen by the index from the virtual disk\n",
        "  response_stu = []\n",
        "  # print(type(disk))\n",
        "  # print(type(diskloc_list))\n",
        "  if len( diskloc_list ) > 0:\n",
        "    response_stu = disk[ diskloc_list ]\n",
        "  df_stu = pd.DataFrame( response_stu, columns = [ \"id\" ] )\n",
        "\n",
        "  # Rename columns just to be safe so as to enable merging\n",
        "  df_stu.rename( dict( zip( df_stu.columns, df_gold.columns ) ), axis = 1, inplace = True )\n",
        "  print(\"My\")\n",
        "  print(df_stu)\n",
        "  print(\"Ans\")\n",
        "  print(df_gold)\n",
        "  \n",
        "  union = pd.merge( df_gold, df_stu, how = \"outer\", indicator = True )\n",
        "  inter = pd.merge( df_gold, df_stu, how = \"inner\", indicator = True )\n",
        "  \n",
        "  # Assuming 'union' already includes the '_merge' column\n",
        "  # difference = union[union['_merge'] != 'both']\n",
        "  # print(difference)\n",
        "\n",
        "\n",
        "\n",
        "  # If the gold response is not empty, use intersection over union score\n",
        "  # Since union removes duplicates, consider length of diskloc_list as well\n",
        "  if len( df_gold ) > 0:\n",
        "    score = round( len( inter ) / max( len( diskloc_list ), len( union ) ), 2 )\n",
        "  # If the gold response itself is empty, penalize non-empty response by index\n",
        "  elif len( df_gold ) == 0:\n",
        "    score = round( 1 / ( 1 + len( diskloc_list ) ), 2 )\n",
        "\n",
        "  # if score != 1:\n",
        "  #   print(clause)\n",
        "\n",
        "  return t_idx, t_seek, t_read, score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KeLLNTmoVB9U"
      },
      "outputs": [],
      "source": [
        "n_trials = 3\n",
        "\n",
        "t_build = 0\n",
        "disk_size = np.int64(0)\n",
        "idx_size = 0\n",
        "t_idx = 0\n",
        "t_seek = np.int64(0)\n",
        "t_read = np.int64(0)\n",
        "score = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8heZOUbvWkbe"
      },
      "outputs": [],
      "source": [
        "# Read the data to be indexed\n",
        "with open( \"public.csv\", 'r' ) as csvfile:\n",
        "  reader = csv.reader( csvfile )\n",
        "  tuples = [ ( int( row[ 0 ] ), row[ 1 ], int( row[ 2 ] ) ) for row in reader ]\n",
        "\n",
        "# Create proper predicates out of CSV data\n",
        "def make_predicates( tok_list ):\n",
        "  if len( tok_list ) == 3:\n",
        "    return [ tok_list ]\n",
        "  if len( tok_list ) == 6:\n",
        "    return [ tok_list[ :3 ], tok_list[ 3: ] ]\n",
        "\n",
        "# Read the clauses that will constitute the evaluation queries\n",
        "with open( \"clauses.csv\", 'r' ) as csvfile:\n",
        "  reader = csv.reader( csvfile )\n",
        "  c_list = [ make_predicates( row ) for row in reader ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PiB-skm2VXFZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m clause \u001b[38;5;129;01min\u001b[39;00m c_list:\n\u001b[0;32m     17\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(clause)\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m     18\u001b[0m   \u001b[38;5;66;03m#   print(clause)\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     t_i, t_s, t_r, scr \u001b[38;5;241m=\u001b[39m \u001b[43meval_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mclause\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx_stat\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     t_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m t_i\n\u001b[0;32m     21\u001b[0m     t_seek \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m t_s\n",
            "Cell \u001b[1;32mIn[3], line 12\u001b[0m, in \u001b[0;36meval_results\u001b[1;34m(clause, disk, idx_stat)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Execute the query using the index and time it\u001b[39;00m\n\u001b[0;32m     11\u001b[0m tic \u001b[38;5;241m=\u001b[39m tm\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m---> 12\u001b[0m diskloc_list \u001b[38;5;241m=\u001b[39m \u001b[43mmy_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mclause\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx_stat\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m toc \u001b[38;5;241m=\u001b[39m tm\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m     14\u001b[0m t_idx \u001b[38;5;241m=\u001b[39m toc \u001b[38;5;241m-\u001b[39m tic\n",
            "File \u001b[1;32mc:\\Shubham\\B-Tech\\Study\\Academics\\Semester 7\\CS 315\\Assign\\CS315-A1\\array\\execute.py:134\u001b[0m, in \u001b[0;36mmy_execute\u001b[1;34m(query, idx_stat)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    133\u001b[0m     prefix \u001b[38;5;241m=\u001b[39m value[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 134\u001b[0m     \u001b[43midx_stat\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43marrIdx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_prefix_match_year\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m     idx_stat[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marrIdx\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mget_name_match_year(year, value)\n",
            "File \u001b[1;32mc:\\Shubham\\B-Tech\\Study\\Academics\\Semester 7\\CS 315\\Assign\\CS315-A1\\array\\index.py:78\u001b[0m, in \u001b[0;36mArrayIndex.get_prefix_match_year\u001b[1;34m(self, year, prefix)\u001b[0m\n\u001b[0;32m     76\u001b[0m     start_ix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_ix[year_ix]\n\u001b[0;32m     77\u001b[0m     end_ix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_ix[year_ix]\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_prefix_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_ix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_ix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n",
            "File \u001b[1;32mc:\\Shubham\\B-Tech\\Study\\Academics\\Semester 7\\CS 315\\Assign\\CS315-A1\\array\\index.py:43\u001b[0m, in \u001b[0;36mStringIndex.get_prefix_match\u001b[1;34m(self, prefix, start_ix, end_ix)\u001b[0m\n\u001b[0;32m     41\u001b[0m left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary_search_left(prefix, start_ix, end_ix)\n\u001b[0;32m     42\u001b[0m right \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary_search_right(prefix\u001b[38;5;241m.\u001b[39mljust(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m'\u001b[39m), start_ix, end_ix)\n\u001b[1;32m---> 43\u001b[0m temp \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [ix \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffset \u001b[38;5;28;01mfor\u001b[39;00m ix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(left, right)]\n",
            "File \u001b[1;32mc:\\Shubham\\B-Tech\\Study\\Academics\\Semester 7\\CS 315\\Assign\\CS315-A1\\array\\index.py:43\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     41\u001b[0m left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary_search_left(prefix, start_ix, end_ix)\n\u001b[0;32m     42\u001b[0m right \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary_search_right(prefix\u001b[38;5;241m.\u001b[39mljust(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m'\u001b[39m), start_ix, end_ix)\n\u001b[1;32m---> 43\u001b[0m temp \u001b[38;5;241m=\u001b[39m [ix \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffset \u001b[38;5;28;01mfor\u001b[39;00m ix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(left, right)]\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [ix \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffset \u001b[38;5;28;01mfor\u001b[39;00m ix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(left, right)]\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for t in range( n_trials ):\n",
        "  my=0\n",
        "  tic = tm.perf_counter()\n",
        "  disk, idx_stat = my_index( tuples )\n",
        "  disk = np.array( disk )\n",
        "  toc = tm.perf_counter()\n",
        "  t_build += toc - tic\n",
        "\n",
        "  disk_size += len( disk )\n",
        "\n",
        "  with open( f\"idx_dump_{t}.pkl\", \"wb\" ) as outfile:\n",
        "    pickle.dump( idx_stat, outfile, protocol=pickle.HIGHEST_PROTOCOL )\n",
        "\n",
        "  idx_size += os.path.getsize( f\"idx_dump_{t}.pkl\" )\n",
        "  # print(idx_stat)\n",
        "  for clause in c_list:\n",
        "    if len(clause)==2:\n",
        "    #   print(clause)\n",
        "      t_i, t_s, t_r, scr = eval_results( clause, disk, idx_stat )\n",
        "      t_idx += t_i\n",
        "      t_seek += t_s\n",
        "      t_read += t_r\n",
        "      score += scr\n",
        "      my+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXvUu6PhW8p_",
        "outputId": "edae3aba-c5f4-4228-c019-8f81f4d6582f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0127994666678812 200000.0 1662898.0 3.8862554666605624 0.0 0.0 0.16666666666666666\n"
          ]
        }
      ],
      "source": [
        "t_build /= n_trials\n",
        "disk_size /= n_trials\n",
        "idx_size /= n_trials\n",
        "t_idx /= n_trials\n",
        "t_seek /= n_trials\n",
        "t_read /= n_trials\n",
        "score /= n_trials\n",
        "score /= my\n",
        "\n",
        "print( t_build, disk_size, idx_size, t_idx, t_seek, t_read, score )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "0.22813676666313162 300000.0 2861000.0 3.787527566673816 6400303.0 1400713.0 1.0\n",
        "0.21858643333447011 300000.0 2861000.0 3.9164780333449016 6400303.0 1400713.0 1.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Disk size is 300000 because ids are stored 3 times: sorted by id, by name, by year\n",
        "2. Index size is high because we have stored entire tuples in idx_stat returned by my_index()\n",
        "3. Score is 1.0 indicating 100% accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ----------------- END ------------------\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below cells are for individual clause testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ind_clause = [['name', 'LIKE', \"'je%'\"], ['year', '>=', '1955']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "My\n",
            "Empty DataFrame\n",
            "Columns: [id]\n",
            "Index: []\n",
            "Ans\n",
            "          id\n",
            "0     100018\n",
            "1     100046\n",
            "2     100117\n",
            "3     100154\n",
            "4     100224\n",
            "...      ...\n",
            "4943  499678\n",
            "4944  499691\n",
            "4945  499758\n",
            "4946  499827\n",
            "4947  499993\n",
            "\n",
            "[4948 rows x 1 columns]\n",
            "0 0 0 0.24363289999746485 0 0 0.0\n"
          ]
        }
      ],
      "source": [
        "import sqlite3 as sql\n",
        "from contextlib import closing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time as tm\n",
        "import pickle\n",
        "import os\n",
        "import csv\n",
        "import importlib\n",
        "import execute  # Import module without specifying the function\n",
        "importlib.reload(execute)  # Force reload\n",
        "import index  # Import module without specifying the function\n",
        "importlib.reload(index) # Force reload\n",
        "from execute import my_execute\n",
        "from index import my_index\n",
        "\n",
        "t_build = 0\n",
        "disk_size = np.int64(0)\n",
        "idx_size = 0\n",
        "t_idx = 0\n",
        "t_seek = np.int64(0)\n",
        "t_read = np.int64(0)\n",
        "score = 0\n",
        "\n",
        "t_i, t_s, t_r, scr = eval_results( ind_clause , disk, idx_stat )\n",
        "t_idx += t_i\n",
        "t_seek += t_s\n",
        "t_read += t_r\n",
        "score += scr\n",
        "\n",
        "print( t_build, disk_size, idx_size, t_idx, t_seek, t_read, score )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below cells are to check disk locations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([491533, 252307, 307789, ..., 194617, 142226, 105687])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query 2 Disk Locations: [141896, 141897, 141898, 141899, 141900, 141901, 141902, 141903, 141904, 141905, 141906, 141907, 141908, 141909, 141910]\n"
          ]
        }
      ],
      "source": [
        "disk_locations2 = my_execute([['name', '=', \"'jafif'\"]], idx_stat)\n",
        "print(\"Query 2 Disk Locations:\", disk_locations2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
