{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "LciKvMEzH9I-"
      },
      "outputs": [],
      "source": [
        "import sqlite3 as sql\n",
        "from contextlib import closing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time as tm\n",
        "import pickle\n",
        "import os\n",
        "import csv\n",
        "import importlib\n",
        "import execute  # Import module without specifying the function\n",
        "importlib.reload(execute)  # Force reload\n",
        "import index  # Import module without specifying the function\n",
        "importlib.reload(index) # Force reload\n",
        "from execute import my_execute\n",
        "from index import my_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGdRLObRKAlM"
      },
      "source": [
        "sqlite3 does not support contextual closing natively (yet). Using a super-elegant workaround proposed by erlendaasland\\\n",
        "https://discuss.python.org/t/implicitly-close-sqlite3-connections-with-context-managers/33320/3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "POoHIPqtIbVz"
      },
      "outputs": [],
      "source": [
        "def safe_tran( db_name, query ):\n",
        "  with closing( sql.connect( db_name ) ) as conn:\n",
        "    cur = conn.execute( query )\n",
        "    cols = [ col[0] for col in cur.description ]\n",
        "    df = pd.DataFrame.from_records( cur, columns = cols )\n",
        "    return df\n",
        "\n",
        "\n",
        "db_name = \"public.db\"\n",
        "get_gold_results = lambda query: safe_tran( db_name, query )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "EY9Q-50DKTdE"
      },
      "outputs": [],
      "source": [
        "def make_sqlite_query( clause ):\n",
        "  query = \"SELECT id FROM tbl WHERE \"\n",
        "  query += \" AND \".join( [ ' '.join( pred ) for pred in clause ] )\n",
        "  return query\n",
        "\n",
        "def eval_results( clause, disk, idx_stat ):\n",
        "  # Execute the query on an actual DB\n",
        "  df_gold = get_gold_results( make_sqlite_query( clause ) )\n",
        "\n",
        "  # Execute the query using the index and time it\n",
        "  tic = tm.perf_counter()\n",
        "  diskloc_list = my_execute( clause, idx_stat )\n",
        "  toc = tm.perf_counter()\n",
        "  t_idx = toc - tic\n",
        "\n",
        "  # Do sanity checks on the returned locations -- dont want any buffer overflow attacks :)\n",
        "  diskloc_list = np.minimum( np.maximum( diskloc_list, 0 ), len( disk ) - 1 )\n",
        "\n",
        "  # Find the seek and read time requried to retrieve records from the virtual disk\n",
        "  diffs = diskloc_list[ 1: ] - diskloc_list[ :-1 ]\n",
        "  # Take care of cases where we need to loop back to reach a record\n",
        "  diffs[ diffs <= 0 ] += len( disk )\n",
        "  t_seek = diffs.sum()\n",
        "  t_read = len( diskloc_list )\n",
        "  # Sanity check\n",
        "  assert( t_seek >= t_read - 1 )\n",
        "  t_seek -= t_read - 1\n",
        "  # Take care of pesky edge cases\n",
        "  if t_read == 0:\n",
        "    t_seek = 0\n",
        "\n",
        "  # Get hold of the tuples chosen by the index from the virtual disk\n",
        "  response_stu = []\n",
        "  # print(type(disk))\n",
        "  # print(type(diskloc_list))\n",
        "  if len( diskloc_list ) > 0:\n",
        "    response_stu = disk[ diskloc_list ]\n",
        "  df_stu = pd.DataFrame( response_stu, columns = [ \"id\" ] )\n",
        "\n",
        "  # Rename columns just to be safe so as to enable merging\n",
        "  df_stu.rename( dict( zip( df_stu.columns, df_gold.columns ) ), axis = 1, inplace = True )\n",
        "  \n",
        "  \n",
        "  union = pd.merge( df_gold, df_stu, how = \"outer\", indicator = True )\n",
        "  inter = pd.merge( df_gold, df_stu, how = \"inner\", indicator = True )\n",
        "  \n",
        "  # Assuming 'union' already includes the '_merge' column\n",
        "  # difference = union[union['_merge'] != 'both']\n",
        "  # print(difference)\n",
        "\n",
        "\n",
        "\n",
        "  # If the gold response is not empty, use intersection over union score\n",
        "  # Since union removes duplicates, consider length of diskloc_list as well\n",
        "  if len( df_gold ) > 0:\n",
        "    score = round( len( inter ) / max( len( diskloc_list ), len( union ) ), 2 )\n",
        "  # If the gold response itself is empty, penalize non-empty response by index\n",
        "  elif len( df_gold ) == 0:\n",
        "    score = round( 1 / ( 1 + len( diskloc_list ) ), 2 )\n",
        "\n",
        "  # if score != 1:\n",
        "  #   print(clause)\n",
        "\n",
        "  return t_idx, t_seek, t_read, score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "KeLLNTmoVB9U"
      },
      "outputs": [],
      "source": [
        "n_trials = 3\n",
        "\n",
        "t_build = 0\n",
        "disk_size = np.int64(0)\n",
        "idx_size = 0\n",
        "t_idx = 0\n",
        "t_seek = np.int64(0)\n",
        "t_read = np.int64(0)\n",
        "score = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "8heZOUbvWkbe"
      },
      "outputs": [],
      "source": [
        "# Read the data to be indexed\n",
        "with open( \"public.csv\", 'r' ) as csvfile:\n",
        "  reader = csv.reader( csvfile )\n",
        "  tuples = [ ( int( row[ 0 ] ), row[ 1 ], int( row[ 2 ] ) ) for row in reader ]\n",
        "\n",
        "# Create proper predicates out of CSV data\n",
        "def make_predicates( tok_list ):\n",
        "  if len( tok_list ) == 3:\n",
        "    return [ tok_list ]\n",
        "  if len( tok_list ) == 6:\n",
        "    return [ tok_list[ :3 ], tok_list[ 3: ] ]\n",
        "\n",
        "# Read the clauses that will constitute the evaluation queries\n",
        "with open( \"clauses.csv\", 'r' ) as csvfile:\n",
        "  reader = csv.reader( csvfile )\n",
        "  c_list = [ make_predicates( row ) for row in reader ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "PiB-skm2VXFZ"
      },
      "outputs": [],
      "source": [
        "for t in range( n_trials ):\n",
        "  tic = tm.perf_counter()\n",
        "  disk, idx_stat = my_index( tuples )\n",
        "  disk = np.array( disk )\n",
        "  toc = tm.perf_counter()\n",
        "  t_build += toc - tic\n",
        "\n",
        "  disk_size += len( disk )\n",
        "\n",
        "  with open( f\"idx_dump_{t}.pkl\", \"wb\" ) as outfile:\n",
        "    pickle.dump( idx_stat, outfile, protocol=pickle.HIGHEST_PROTOCOL )\n",
        "\n",
        "  idx_size += os.path.getsize( f\"idx_dump_{t}.pkl\" )\n",
        "  # print(idx_stat)\n",
        "  for clause in c_list:\n",
        "    t_i, t_s, t_r, scr = eval_results( clause, disk, idx_stat )\n",
        "    t_idx += t_i\n",
        "    t_seek += t_s\n",
        "    t_read += t_r\n",
        "    score += scr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXvUu6PhW8p_",
        "outputId": "edae3aba-c5f4-4228-c019-8f81f4d6582f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.8126146666666805 200000.0 20771422.0 0.035879766666084834 2227760.0 1400713.0 1.0\n"
          ]
        }
      ],
      "source": [
        "t_build /= n_trials\n",
        "disk_size /= n_trials\n",
        "idx_size /= n_trials\n",
        "t_idx /= n_trials\n",
        "t_seek /= n_trials\n",
        "t_read /= n_trials\n",
        "score /= n_trials\n",
        "score /= len( c_list )\n",
        "\n",
        "print( t_build, disk_size, idx_size, t_idx, t_seek, t_read, score )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "0.22813676666313162 300000.0 2861000.0 3.787527566673816 6400303.0 1400713.0 1.0\n",
        "0.21858643333447011 300000.0 2861000.0 3.9164780333449016 6400303.0 1400713.0 1.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Disk size is 300000 because ids are stored 3 times: sorted by id, by name, by year\n",
        "2. Index size is high because we have stored entire tuples in idx_stat returned by my_index()\n",
        "3. Score is 1.0 indicating 100% accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ----------------- END ------------------\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below cells are for individual clause testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "ind_clause = [['name', 'LIKE', \"'je%'\"], ['year', '>=', '1955']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024}\n",
            "1964 range(13, 14)\n",
            "1965 range(37, 39)\n",
            "1966 range(63, 64)\n",
            "1968 range(172, 176)\n",
            "1969 range(255, 261)\n",
            "1970 range(359, 362)\n",
            "1971 range(475, 477)\n",
            "1972 range(615, 620)\n",
            "1973 range(797, 809)\n",
            "1974 range(1009, 1015)\n",
            "1975 range(1256, 1267)\n",
            "1976 range(1531, 1540)\n",
            "1977 range(1877, 1889)\n",
            "1978 range(2289, 2309)\n",
            "1979 range(2766, 2786)\n",
            "1980 range(3341, 3375)\n",
            "1981 range(3992, 4019)\n",
            "1982 range(4692, 4727)\n",
            "1983 range(5481, 5528)\n",
            "1984 range(6397, 6442)\n",
            "1985 range(7406, 7451)\n",
            "1986 range(8564, 8623)\n",
            "1987 range(9796, 9863)\n",
            "1988 range(11165, 11233)\n",
            "1989 range(12691, 12757)\n",
            "1990 range(14282, 14361)\n",
            "1991 range(15918, 16017)\n",
            "1992 range(17784, 17881)\n",
            "1993 range(19701, 19791)\n",
            "1994 range(21734, 21853)\n",
            "1995 range(23802, 23904)\n",
            "1996 range(25923, 26026)\n",
            "1997 range(28101, 28233)\n",
            "1998 range(30441, 30560)\n",
            "1999 range(32891, 33031)\n",
            "2000 range(35204, 35318)\n",
            "2001 range(37497, 37613)\n",
            "2002 range(39835, 39953)\n",
            "2003 range(42183, 42303)\n",
            "2004 range(44417, 44519)\n",
            "2005 range(46584, 46695)\n",
            "2006 range(48730, 48821)\n",
            "2007 range(50754, 50871)\n",
            "2008 range(52770, 52871)\n",
            "2009 range(54668, 54769)\n",
            "2010 range(56505, 56591)\n",
            "2011 range(58428, 58520)\n",
            "2012 range(60329, 60422)\n",
            "2013 range(62348, 62449)\n",
            "2014 range(64562, 64686)\n",
            "2015 range(67063, 67196)\n",
            "2016 range(69911, 70069)\n",
            "2017 range(73341, 73507)\n",
            "2018 range(76953, 77127)\n",
            "2019 range(80826, 81017)\n",
            "2020 range(84933, 85139)\n",
            "2021 range(88978, 89164)\n",
            "2022 range(92711, 92887)\n",
            "2023 range(95993, 96135)\n",
            "2024 range(98674, 98816)\n",
            "Empty DataFrame\n",
            "Columns: [id, _merge]\n",
            "Index: []\n",
            "0 0 0 0.001055400000041118 93855 4948 1.0\n"
          ]
        }
      ],
      "source": [
        "import sqlite3 as sql\n",
        "from contextlib import closing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time as tm\n",
        "import pickle\n",
        "import os\n",
        "import csv\n",
        "import importlib\n",
        "import execute  # Import module without specifying the function\n",
        "importlib.reload(execute)  # Force reload\n",
        "import index  # Import module without specifying the function\n",
        "importlib.reload(index) # Force reload\n",
        "from execute import my_execute\n",
        "from index import my_index\n",
        "\n",
        "t_build = 0\n",
        "disk_size = np.int64(0)\n",
        "idx_size = 0\n",
        "t_idx = 0\n",
        "t_seek = np.int64(0)\n",
        "t_read = np.int64(0)\n",
        "score = 0\n",
        "\n",
        "t_i, t_s, t_r, scr = eval_results( ind_clause , disk, idx_stat )\n",
        "t_idx += t_i\n",
        "t_seek += t_s\n",
        "t_read += t_r\n",
        "score += scr\n",
        "\n",
        "print( t_build, disk_size, idx_size, t_idx, t_seek, t_read, score )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below cells are to check disk locations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([491533, 252307, 307789, ..., 194617, 142226, 105687])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['name', '=', \"'jafif'\"]]\n",
            "j\n",
            "a\n",
            "f\n",
            "i\n",
            "f\n",
            "Query 2 Disk Locations: [141896, 141897, 141898, 141899, 141900, 141901, 141902, 141903, 141904, 141905, 141906, 141907, 141908, 141909, 141910]\n"
          ]
        }
      ],
      "source": [
        "disk_locations2 = my_execute([['name', '=', \"'jafif'\"]], idx_stat)\n",
        "print(\"Query 2 Disk Locations:\", disk_locations2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
