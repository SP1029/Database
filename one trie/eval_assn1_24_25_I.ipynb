{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "LciKvMEzH9I-"
   },
   "outputs": [],
   "source": [
    "import sqlite3 as sql\n",
    "from contextlib import closing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time as tm\n",
    "import pickle\n",
    "import os\n",
    "import csv\n",
    "import importlib\n",
    "import execute  # Import module without specifying the function\n",
    "importlib.reload(execute)  # Force reload\n",
    "import index  # Import module without specifying the function\n",
    "importlib.reload(index) # Force reload\n",
    "from execute import my_execute\n",
    "from index import my_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGdRLObRKAlM"
   },
   "source": [
    "sqlite3 does not support contextual closing natively (yet). Using a super-elegant workaround proposed by erlendaasland\\\n",
    "https://discuss.python.org/t/implicitly-close-sqlite3-connections-with-context-managers/33320/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "POoHIPqtIbVz"
   },
   "outputs": [],
   "source": [
    "def safe_tran( db_name, query ):\n",
    "  with closing( sql.connect( db_name ) ) as conn:\n",
    "    cur = conn.execute( query )\n",
    "    cols = [ col[0] for col in cur.description ]\n",
    "    df = pd.DataFrame.from_records( cur, columns = cols )\n",
    "    return df\n",
    "\n",
    "\n",
    "db_name = \"public.db\"\n",
    "get_gold_results = lambda query: safe_tran( db_name, query )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "EY9Q-50DKTdE"
   },
   "outputs": [],
   "source": [
    "def make_sqlite_query( clause ):\n",
    "  query = \"SELECT id FROM tbl WHERE \"\n",
    "  query += \" AND \".join( [ ' '.join( pred ) for pred in clause ] )\n",
    "  return query\n",
    "\n",
    "def eval_results( clause, disk, idx_stat ):\n",
    "  # Execute the query on an actual DB\n",
    "  df_gold = get_gold_results( make_sqlite_query( clause ) )\n",
    "\n",
    "  # Execute the query using the index and time it\n",
    "  tic = tm.perf_counter()\n",
    "  diskloc_list = my_execute( clause, idx_stat )\n",
    "  toc = tm.perf_counter()\n",
    "  t_idx = toc - tic\n",
    "\n",
    "  # Do sanity checks on the returned locations -- dont want any buffer overflow attacks :)\n",
    "  diskloc_list = np.minimum( np.maximum( diskloc_list, 0 ), len( disk ) - 1 )\n",
    "\n",
    "  # Find the seek and read time requried to retrieve records from the virtual disk\n",
    "  diffs = diskloc_list[ 1: ] - diskloc_list[ :-1 ]\n",
    "  # Take care of cases where we need to loop back to reach a record\n",
    "  diffs[ diffs <= 0 ] += len( disk )\n",
    "  t_seek = diffs.sum()\n",
    "  t_read = len( diskloc_list )\n",
    "  # Sanity check\n",
    "  assert( t_seek >= t_read - 1 )\n",
    "  t_seek -= t_read - 1\n",
    "  # Take care of pesky edge cases\n",
    "  if t_read == 0:\n",
    "    t_seek = 0\n",
    "\n",
    "  # Get hold of the tuples chosen by the index from the virtual disk\n",
    "  response_stu = []\n",
    "  # print(type(disk))\n",
    "  # print(type(diskloc_list))\n",
    "  if len( diskloc_list ) > 0:\n",
    "    response_stu = disk[ diskloc_list ]\n",
    "  df_stu = pd.DataFrame( response_stu, columns = [ \"id\" ] )\n",
    "\n",
    "  # Rename columns just to be safe so as to enable merging\n",
    "  df_stu.rename( dict( zip( df_stu.columns, df_gold.columns ) ), axis = 1, inplace = True )\n",
    "  \n",
    "  \n",
    "  union = pd.merge( df_gold, df_stu, how = \"outer\", indicator = True )\n",
    "  inter = pd.merge( df_gold, df_stu, how = \"inner\", indicator = True )\n",
    "  \n",
    "  # Assuming 'union' already includes the '_merge' column\n",
    "  # difference = union[union['_merge'] != 'both']\n",
    "  # print(difference)\n",
    "\n",
    "\n",
    "\n",
    "  # If the gold response is not empty, use intersection over union score\n",
    "  # Since union removes duplicates, consider length of diskloc_list as well\n",
    "  if len( df_gold ) > 0:\n",
    "    score = round( len( inter ) / max( len( diskloc_list ), len( union ) ), 2 )\n",
    "  # If the gold response itself is empty, penalize non-empty response by index\n",
    "  elif len( df_gold ) == 0:\n",
    "    score = round( 1 / ( 1 + len( diskloc_list ) ), 2 )\n",
    "\n",
    "  # if score != 1:\n",
    "  #   print(clause)\n",
    "\n",
    "  return t_idx, t_seek, t_read, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "KeLLNTmoVB9U"
   },
   "outputs": [],
   "source": [
    "n_trials = 3\n",
    "\n",
    "t_build = 0\n",
    "disk_size = np.int64(0)\n",
    "idx_size = 0\n",
    "t_idx = 0\n",
    "t_seek = np.int64(0)\n",
    "t_read = np.int64(0)\n",
    "score = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "8heZOUbvWkbe"
   },
   "outputs": [],
   "source": [
    "# Read the data to be indexed\n",
    "with open( \"public.csv\", 'r' ) as csvfile:\n",
    "  reader = csv.reader( csvfile )\n",
    "  tuples = [ ( int( row[ 0 ] ), row[ 1 ], int( row[ 2 ] ) ) for row in reader ]\n",
    "\n",
    "# Create proper predicates out of CSV data\n",
    "def make_predicates( tok_list ):\n",
    "  if len( tok_list ) == 3:\n",
    "    return [ tok_list ]\n",
    "  if len( tok_list ) == 6:\n",
    "    return [ tok_list[ :3 ], tok_list[ 3: ] ]\n",
    "\n",
    "# Read the clauses that will constitute the evaluation queries\n",
    "with open( \"clauses.csv\", 'r' ) as csvfile:\n",
    "  reader = csv.reader( csvfile )\n",
    "  c_list = [ make_predicates( row ) for row in reader ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "PiB-skm2VXFZ"
   },
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "no such table: tbl",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# print(idx_stat)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m clause \u001b[38;5;129;01min\u001b[39;00m c_list:\n\u001b[1;32m---> 16\u001b[0m   t_i, t_s, t_r, scr \u001b[38;5;241m=\u001b[39m \u001b[43meval_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mclause\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx_stat\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m   t_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m t_i\n\u001b[0;32m     18\u001b[0m   t_seek \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m t_s\n",
      "Cell \u001b[1;32mIn[15], line 8\u001b[0m, in \u001b[0;36meval_results\u001b[1;34m(clause, disk, idx_stat)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval_results\u001b[39m( clause, disk, idx_stat ):\n\u001b[0;32m      7\u001b[0m   \u001b[38;5;66;03m# Execute the query on an actual DB\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m   df_gold \u001b[38;5;241m=\u001b[39m \u001b[43mget_gold_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mmake_sqlite_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mclause\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m   \u001b[38;5;66;03m# Execute the query using the index and time it\u001b[39;00m\n\u001b[0;32m     11\u001b[0m   tic \u001b[38;5;241m=\u001b[39m tm\u001b[38;5;241m.\u001b[39mperf_counter()\n",
      "Cell \u001b[1;32mIn[14], line 10\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m      9\u001b[0m db_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpublic.db\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 10\u001b[0m get_gold_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m query: \u001b[43msafe_tran\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m, in \u001b[0;36msafe_tran\u001b[1;34m(db_name, query)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msafe_tran\u001b[39m( db_name, query ):\n\u001b[0;32m      2\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m closing( sql\u001b[38;5;241m.\u001b[39mconnect( db_name ) ) \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[1;32m----> 3\u001b[0m     cur \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     cols \u001b[38;5;241m=\u001b[39m [ col[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cur\u001b[38;5;241m.\u001b[39mdescription ]\n\u001b[0;32m      5\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records( cur, columns \u001b[38;5;241m=\u001b[39m cols )\n",
      "\u001b[1;31mOperationalError\u001b[0m: no such table: tbl"
     ]
    }
   ],
   "source": [
    "for t in range( n_trials ):\n",
    "  tic = tm.perf_counter()\n",
    "  disk, idx_stat = my_index( tuples )\n",
    "  disk = np.array( disk )\n",
    "  toc = tm.perf_counter()\n",
    "  t_build += toc - tic\n",
    "\n",
    "  disk_size += len( disk )\n",
    "\n",
    "  with open( f\"idx_dump_{t}.pkl\", \"wb\" ) as outfile:\n",
    "    pickle.dump( idx_stat, outfile, protocol=pickle.HIGHEST_PROTOCOL )\n",
    "\n",
    "  idx_size += os.path.getsize( f\"idx_dump_{t}.pkl\" )\n",
    "  # print(idx_stat)\n",
    "  for clause in c_list:\n",
    "    t_i, t_s, t_r, scr = eval_results( clause, disk, idx_stat )\n",
    "    t_idx += t_i\n",
    "    t_seek += t_s\n",
    "    t_read += t_r\n",
    "    score += scr\n",
    "    # if scr != 1:\n",
    "    #   print(clause)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tXvUu6PhW8p_",
    "outputId": "edae3aba-c5f4-4228-c019-8f81f4d6582f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'c_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m t_read \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m n_trials\n\u001b[0;32m      7\u001b[0m score \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m n_trials\n\u001b[1;32m----> 8\u001b[0m score \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m( \u001b[43mc_list\u001b[49m )\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m( t_build, disk_size, idx_size, t_idx, t_seek, t_read, score )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'c_list' is not defined"
     ]
    }
   ],
   "source": [
    "t_build /= n_trials\n",
    "disk_size /= n_trials\n",
    "idx_size /= n_trials\n",
    "t_idx /= n_trials\n",
    "t_seek /= n_trials\n",
    "t_read /= n_trials\n",
    "score /= n_trials\n",
    "score /= len( c_list )\n",
    "\n",
    "print( t_build, disk_size, idx_size, t_idx, t_seek, t_read, score )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.22813676666313162 300000.0 2861000.0 3.787527566673816 6400303.0 1400713.0 1.0\n",
    "0.21858643333447011 300000.0 2861000.0 3.9164780333449016 6400303.0 1400713.0 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Disk size is 300000 because ids are stored 3 times: sorted by id, by name, by year\n",
    "2. Index size is high because we have stored entire tuples in idx_stat returned by my_index()\n",
    "3. Score is 1.0 indicating 100% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------- END ------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below cells are for individual clause testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_clause = [['name', 'LIKE', \"'so%'\"], ['year', '>=', '1996']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 as sql\n",
    "from contextlib import closing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time as tm\n",
    "import pickle\n",
    "import os\n",
    "import csv\n",
    "import importlib\n",
    "import execute  # Import module without specifying the function\n",
    "importlib.reload(execute)  # Force reload\n",
    "import index  # Import module without specifying the function\n",
    "importlib.reload(index) # Force reload\n",
    "from execute import my_execute\n",
    "from index import my_index\n",
    "\n",
    "t_build = 0\n",
    "disk_size = np.int64(0)\n",
    "idx_size = 0\n",
    "t_idx = 0\n",
    "t_seek = np.int64(0)\n",
    "t_read = np.int64(0)\n",
    "score = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below cells are to check disk locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_i, t_s, t_r, scr = eval_results( ind_clause , disk, idx_stat )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
